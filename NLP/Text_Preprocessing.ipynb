{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNIT9+C66IFWb0oMMCf4D3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HHansi/Applied-AI-Course/blob/main/NLP/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bijgqBg-VMJA"
      },
      "source": [
        "# Tokenisation\r\n",
        "Tokenisation is the process which divides text into smaller parts called tokens.\r\n",
        "\r\n",
        "Let's see how to use [tokenizers](https://www.nltk.org/api/nltk.tokenize.html) available with NLTK (Natural Language Toolkit) package to tokenise text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF5HAhIugmwP"
      },
      "source": [
        "sample_text = \"This is a sentence, which contains all kind of 'words', and needs to be tokenized!\"\r\n",
        "sample_tweet1 = \"This is a cooool :-) :-P <3 #cool\"\r\n",
        "sample_tweet2 = \"@remy: This is waaaaayyyy too much for you!!!!!!\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj8A08V7h1ir"
      },
      "source": [
        "Tokenising normal text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40Dcb7ITfT2y",
        "outputId": "ac9c6c60-4b2f-4ff4-b1f6-4fa8dc84fbf4"
      },
      "source": [
        "# download the module 'punkt'\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "tokenized_text = word_tokenize(sample_text)\r\n",
        "print(tokenized_text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['This', 'is', 'a', 'sentence', ',', 'which', 'contains', 'all', 'kind', 'of', \"'words\", \"'\", ',', 'and', 'needs', 'to', 'be', 'tokenized', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PlF7OQh73p"
      },
      "source": [
        "Tokenising tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAtt6c63VOU5",
        "outputId": "bdeaf437-249a-44b9-94a0-7caa05aa868e"
      },
      "source": [
        "tokenized_tweet1 = word_tokenize(sample_tweet1)\r\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\r\n",
        "\r\n",
        "tokenized_tweet2 = word_tokenize(sample_tweet2)\r\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized tweet1: ['This', 'is', 'a', 'cooool', ':', '-', ')', ':', '-P', '<', '3', '#', 'cool']\n",
            "tokenized tweet2: ['@', 'remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!', '!', '!', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3MnOMdrh-da"
      },
      "source": [
        "As you can see in the above outputs, <i>word_tokenize</i> cannot tokenize the tweet text correctly. \r\n",
        "Considering the differences in tweet text compared to normal text, there is an another tokenizer named <i>TweetTokenizer</i> available with NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8LHQrQrfllV",
        "outputId": "d0a09c77-28e7-41ea-ee3f-f267d819d457"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\r\n",
        "tknzr = TweetTokenizer()\r\n",
        "\r\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\r\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\r\n",
        "\r\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\r\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized tweet1: ['This', 'is', 'a', 'cooool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-KNtZ6LjSvA"
      },
      "source": [
        "Let us analyse more features available with TweetTokenizer.\r\n",
        "- preserve_case (default setting=True) - Keep case sensitivity of the text\r\n",
        "- reduce_len (default setting=False) - Normalize text by removing repeated character sequences of length 3 or greater with sequences of length 3.\r\n",
        "- strip_handles (default setting=False) - Remove Twitter usernames in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv2rAXcHXqON",
        "outputId": "cb4a85f6-b58c-43d7-8bf4-6db65eaec027"
      },
      "source": [
        "# make the tokens case insensitive or convert into lowercase\r\n",
        "print('configs: preserve_case=False')\r\n",
        "tknzr = TweetTokenizer(preserve_case=False)\r\n",
        "\r\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\r\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\r\n",
        "\r\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\r\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')\r\n",
        "\r\n",
        "\r\n",
        "# make the tokens case insensitive and reduce length\r\n",
        "print('\\nconfigs: preserve_case=False, reduce_len=True')\r\n",
        "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True)\r\n",
        "\r\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\r\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\r\n",
        "\r\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\r\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')\r\n",
        "\r\n",
        "\r\n",
        "# make the tokens case insensitive, reduce length and remove usernames\r\n",
        "print('\\nconfigs: preserve_case=False, reduce_len=True, strip_handles=True')\r\n",
        "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\r\n",
        "\r\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\r\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\r\n",
        "\r\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\r\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "configs: preserve_case=False\n",
            "tokenized tweet1: ['this', 'is', 'a', 'cooool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'this', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
            "\n",
            "configs: preserve_case=False, reduce_len=True\n",
            "tokenized tweet1: ['this', 'is', 'a', 'coool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'this', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
            "\n",
            "configs: preserve_case=False, reduce_len=True, strip_handles=True\n",
            "tokenized tweet1: ['this', 'is', 'a', 'coool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: [':', 'this', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qkRlhEoH6IC"
      },
      "source": [
        "# Text Normalisation\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oTfsvegJn5C"
      },
      "source": [
        "## Lower casing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1V46nFRRcv",
        "outputId": "10dae8dd-3ee2-4576-ec18-302162f15f57"
      },
      "source": [
        "# Any string can be lower cased using the function lower()\r\n",
        "lower_cased_text = sample_text.lower()\r\n",
        "print(lower_cased_text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is a sentence, which contains all kind of 'words', and needs to be tokenized!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9M2IprIg2H"
      },
      "source": [
        "## Stemming\r\n",
        "Stemming chops off the end or beginning of words by taking into account a list of common prefixes or suffixes that could be found in that word.\r\n",
        "\r\n",
        "The most common and effecive algorithm for stemming English is <i>Porter’s algorithm.</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vIsy8ZcIftF",
        "outputId": "ad4304cb-0e41-493f-8b5a-4d6e55b76102"
      },
      "source": [
        "from nltk.stem import PorterStemmer\r\n",
        "\r\n",
        "ps = PorterStemmer()\r\n",
        "\r\n",
        "sample_words = [\"ponies\", \"cats\", \"eating\", \"ate\", \"eat\", \"goose\", \"geese\"]\r\n",
        "\r\n",
        "stem_words = []\r\n",
        "for word in sample_words:\r\n",
        "    stem_word=ps.stem(word)\r\n",
        "    stem_words.append(stem_word)\r\n",
        "  \r\n",
        "print(f'Stemmed words: {stem_words}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemmed words: ['poni', 'cat', 'eat', 'ate', 'eat', 'goos', 'gees']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsDEnGi2I4GT"
      },
      "source": [
        "## Lemmatisation\r\n",
        "\r\n",
        "Lemmatisation is an more organised procedure to obtain the base form of a word (lemma) with the use of a vocabulary and morphological analysis (word structure and grammar relations) of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWZTVWOvH-5m",
        "outputId": "5d84c1f7-eaef-4433-9112-639eca3d4df3"
      },
      "source": [
        "# download the module 'wordnet'\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "\r\n",
        "wnl = WordNetLemmatizer()\r\n",
        "\r\n",
        "sample_words = [\"ponies\", \"cats\", \"eating\", \"ate\", \"eat\", \"goose\", \"geese\"]\r\n",
        "\r\n",
        "lemma_words = []\r\n",
        "for word in sample_words:\r\n",
        "    lemma_word=wnl.lemmatize(word)\r\n",
        "    lemma_words.append(lemma_word)\r\n",
        "  \r\n",
        "print(f'Lemmatised words: {lemma_words}')\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Lemmatised words: ['pony', 'cat', 'eating', 'ate', 'eat', 'goose', 'goose']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFMZJc5oKZ_8"
      },
      "source": [
        "Compare the difference between the outputs by stemmer and lemmatiser.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBkMybM0JTpQ"
      },
      "source": [
        "<b>Lemmatisation with Part-of-Speech (PoS) tags</b>\r\n",
        "\r\n",
        "[Parts of speech](https://www.englishclub.com/grammar/parts-of-speech.htm) are also known as word classes or lexical categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTrh7VoHJUdo",
        "outputId": "62fdf9f8-374c-4c52-d025-268e6e5f26cd"
      },
      "source": [
        "lemma_word=wnl.lemmatize('ponies', pos='n')\r\n",
        "print(lemma_word)\r\n",
        "\r\n",
        "lemma_word=wnl.lemmatize('eating', pos='v')\r\n",
        "print(lemma_word)\r\n",
        "\r\n",
        "lemma_word=wnl.lemmatize('ate', pos='v')\r\n",
        "print(lemma_word)\r\n",
        "\r\n",
        "lemma_word=wnl.lemmatize('eat', pos='v')\r\n",
        "print(lemma_word)\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pony\n",
            "eat\n",
            "eat\n",
            "eat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUJ6XKBKp-UM"
      },
      "source": [
        "# Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxljnBblp9IS",
        "outputId": "a40b4393-f19b-4274-d017-f47689890b8d"
      },
      "source": [
        "# download the module 'stopwords'\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "# define set of English stopwords\r\n",
        "stop_words = set(stopwords.words('english')) \r\n",
        "\r\n",
        "sample_text = \"This is a sample sentence, showing off the stop words removal.\"\r\n",
        "\r\n",
        "# tokenise text\r\n",
        "tokens = word_tokenize(sample_text)\r\n",
        "\r\n",
        "filtered_words = []\r\n",
        " \r\n",
        "# remove stopwords from tokens\r\n",
        "for token in tokens:\r\n",
        "  if token not in stop_words:\r\n",
        "    filtered_words.append(token)\r\n",
        "print(filtered_words)\r\n",
        "\r\n",
        "# join tokens into a sentence using space\r\n",
        "merged_text = \" \".join(filtered_words)\r\n",
        "print(merged_text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'removal', '.']\n",
            "This sample sentence , showing stop words removal .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OParvOihqaK4"
      },
      "source": [
        "# Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnZQE1FpqZUF",
        "outputId": "270c47b0-2cb2-43d6-bcfa-1f65e4276dbf"
      },
      "source": [
        "import string\r\n",
        "\r\n",
        "sample_text = \"Let's remove punctuation marks!\"\r\n",
        "\r\n",
        "print(f'Punctuation marks to remove: {string.punctuation}')\r\n",
        "\r\n",
        "# remove puncuation marks in sample text\r\n",
        "table = str.maketrans(dict.fromkeys(string.punctuation))\r\n",
        "no_punctuation= sample_text.translate(table)\r\n",
        "\r\n",
        "print(no_punctuation)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Punctuation marks to remove: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "Lets remove punctuation marks\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}