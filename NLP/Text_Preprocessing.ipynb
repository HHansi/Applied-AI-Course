{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HHansi/Applied-AI-Course/blob/main/NLP/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bijgqBg-VMJA"
      },
      "source": [
        "# Text Processing\n",
        "\n",
        "This notebook contains the practical examples and exercises for the Applied AI-Natural Language Processing.\n",
        "\n",
        "*Created by Hansi Hettiarachchi*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenisation\n",
        "Tokenisation is the task of cutting a string into identifiable linguistic units that constitute a piece of language data.\n",
        "\n",
        "Let's see how to use [tokenizers](https://www.nltk.org/api/nltk.tokenize.html) available with NLTK (Natural Language Toolkit) package to tokenise text."
      ],
      "metadata": {
        "id": "0yTL8Gxi6jrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # NLTK module required for Tokenizers\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByV9qPvw16Kc",
        "outputId": "fafaf597-c2f2-4e84-d873-8e0ad688222e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF5HAhIugmwP"
      },
      "source": [
        "sample_text = \"This is a sentence, which contains all kind of words, and needs to be tokenized!\"\n",
        "sample_tweet1 = \"This is a cooool :-) :-P <3 #cool\"\n",
        "sample_tweet2 = \"@remy: This is waaaaayyyy too much for you!!!!!!\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj8A08V7h1ir"
      },
      "source": [
        "Tokenising normal text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40Dcb7ITfT2y",
        "outputId": "e4d1ce3f-513a-4b29-891e-78ae4ffa6e74"
      },
      "source": [
        "tokenized_text = word_tokenize(sample_text)\n",
        "print(tokenized_text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sentence', ',', 'which', 'contains', 'all', 'kind', 'of', 'words', ',', 'and', 'needs', 'to', 'be', 'tokenized', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PlF7OQh73p"
      },
      "source": [
        "Tokenising tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAtt6c63VOU5",
        "outputId": "eaa37b4f-e5a0-4653-d503-e58a9cf78fe5"
      },
      "source": [
        "tokenized_tweet1 = word_tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = word_tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized tweet1: ['This', 'is', 'a', 'cooool', ':', '-', ')', ':', '-P', '<', '3', '#', 'cool']\n",
            "tokenized tweet2: ['@', 'remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3MnOMdrh-da"
      },
      "source": [
        "As you can see in the above outputs, <i>word_tokenize</i> cannot tokenize the tweet text correctly.\n",
        "Considering the differences in tweet text compared to normal text, there is an another tokenizer named <i>TweetTokenizer</i> available with NLTK which is specifically designed for tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8LHQrQrfllV",
        "outputId": "af7f65c2-1692-43fb-ce74-6537d0633cc5"
      },
      "source": [
        "tknzr = TweetTokenizer()\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized tweet1: ['This', 'is', 'a', 'cooool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-KNtZ6LjSvA"
      },
      "source": [
        "Let's analyse more features available with [TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.casual.html?highlight=tweettokenizer#nltk.tokenize.casual.TweetTokenizer).\n",
        "- preserve_case (default setting=True) - Keep case sensitivity of the text\n",
        "- reduce_len (default setting=False) - Normalize text by removing repeated character sequences of length 3 or greater with sequences of length 3.\n",
        "- strip_handles (default setting=False) - Remove Twitter usernames in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv2rAXcHXqON",
        "outputId": "41378edb-02c5-457d-8e32-a8b72eeb873c"
      },
      "source": [
        "# setting1: make the tokens case insensitive or convert into lowercase\n",
        "print('configs: preserve_case=False')\n",
        "tknzr = TweetTokenizer(preserve_case=False)\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "configs: preserve_case=False\n",
            "tokenized tweet1: ['this', 'is', 'a', 'cooool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'this', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting2: make the tokens case insensitive and reduce length\n",
        "print('\\nconfigs: preserve_case=False, reduce_len=True')\n",
        "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz8vi2Bni3Hy",
        "outputId": "053339b0-6fa3-4d6b-fafc-1656bcd81abf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "configs: preserve_case=False, reduce_len=True\n",
            "tokenized tweet1: ['this', 'is', 'a', 'coool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'this', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting3: make the tokens case insensitive, reduce length and remove usernames\n",
        "print('\\nconfigs: preserve_case=False, reduce_len=True, strip_handles=True')\n",
        "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOZWIpAmi4z5",
        "outputId": "63d8df2b-e9b4-44aa-ba00-ad315351fb8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "configs: preserve_case=False, reduce_len=True, strip_handles=True\n",
            "tokenized tweet1: ['this', 'is', 'a', 'coool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: [':', 'this', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='green'>**Activity 1**</font>\n",
        "\n",
        "Analyse the outputs from TweetTokenizer above under settings 1, 2 and 3 and identify the best setting to use, if your final aim is to predict the sentiment (positive, negative and neutral) of the tweet."
      ],
      "metadata": {
        "id": "WGHoIRPamkPS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qkRlhEoH6IC"
      },
      "source": [
        "# Text Normalisation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oTfsvegJn5C"
      },
      "source": [
        "## Lower casing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "6MlkHXNtowNl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"The striped BATs are hanging on their feet for best\""
      ],
      "metadata": {
        "id": "vRRSaEGpnBmg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1V46nFRRcv",
        "outputId": "f0627616-f643-4092-8398-5d3e531eaa88"
      },
      "source": [
        "# Any string can be lower cased using the function lower()\n",
        "lower_cased_text = sample_text.lower()\n",
        "print(lower_cased_text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the striped bats are hanging on their feet for best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are not familiar with string methods, you can find a list of all of them in the [documentation](https://docs.python.org/3.7/library/stdtypes.html#string-methods)."
      ],
      "metadata": {
        "id": "yrVsQP4qi-SX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9M2IprIg2H"
      },
      "source": [
        "## Stemming\n",
        "Stemming chops off the end or beginning of words by taking into account a list of common prefixes or suffixes that could be found in that word.\n",
        "\n",
        "The most common and effecive algorithm for stemming English is <i>Porter’s algorithm.</i>\n",
        "\n",
        "Details of different stemmers available with NLTK is available [here](https://www.nltk.org/howto/stem.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "-jY9nY_ql7Yi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vIsy8ZcIftF",
        "outputId": "24d40317-9a73-40f6-84e6-6876a253d6eb"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "word = \"dogs\"\n",
        "stem_word = ps.stem(word)\n",
        "\n",
        "print(f'Stemmed word: {stem_word}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word: dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have a list of words, you need to iteratively go through each to do the conversion\n",
        "sample_words = [\"dogs\", \"ponies\", \"eating\", \"corpora\"]\n",
        "stem_words = [ps.stem(word) for word in sample_words]\n",
        "\n",
        "print(f'Stemmed words: {stem_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlK4G4ZEkuK4",
        "outputId": "be762f7f-13e5-487e-bc21-5ef227d557c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['dog', 'poni', 'eat', 'corpora']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemmers take a single word as the input. If you have a sentence, you need to first tokenise it."
      ],
      "metadata": {
        "id": "oJKtinMskC3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"The striped bats are hanging on their feet for best.\"\n",
        "\n",
        "tokens = word_tokenize(sample_sentence)\n",
        "stem_words = [ps.stem(word) for word in tokens]\n",
        "\n",
        "print(f'Stemmed words: {stem_words}')"
      ],
      "metadata": {
        "id": "e_nqLwB2pqY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27744a4-554b-4600-caf0-0598f7034e4d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If required, you can also convert the stem words into a sentence by merging them with a space between each word.\n",
        "print(f'Stemmed sentence: {\" \".join(stem_words)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnzjcnC4l3oI",
        "outputId": "afbc4275-b073-4b4a-f02d-aa1d07cc30b9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed sentence: the stripe bat are hang on their feet for best .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsDEnGi2I4GT"
      },
      "source": [
        "## Lemmatisation\n",
        "\n",
        "Lemmatisation is an more organised procedure to obtain the base form of a word (lemma) with the use of a vocabulary and morphological analysis (word structure and grammar relations) of words."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.WordNetLemmatizer.lemmatize)"
      ],
      "metadata": {
        "id": "hQx5xW5NkKWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')  # NLTK module required for WordNetLemmatizer\n",
        "nltk.download('omw-1.4') # NLTK module required for WordNetLemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8BhOdEgq53x",
        "outputId": "92aaab37-c4fc-4700-b8c7-24787b7a4e71"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWZTVWOvH-5m",
        "outputId": "1b0bb0c1-144a-4625-b197-aac06bc270c7"
      },
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "word = \"dogs\"\n",
        "lemma_word = wnl.lemmatize(word)\n",
        "\n",
        "print(f'Lemmatised word: {lemma_word}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised word: dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have a list of words, you need to iteratively go through each to do the conversion"
      ],
      "metadata": {
        "id": "OJK7yMGmxO-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = [\"dogs\", \"ponies\", \"eating\", \"corpora\"]\n",
        "lemma_words = [wnl.lemmatize(word) for word in sample_words]\n",
        "\n",
        "print(f'Lemmatised words: {lemma_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvHh-_nJpz4E",
        "outputId": "8c7b44bc-1b0d-4843-803d-35e1ba70e2a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised words: ['dog', 'pony', 'eating', 'corpus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to Stemmmers, Lemmatizers also take a single word as the input. If you have a sentence, you need to first tokenise it."
      ],
      "metadata": {
        "id": "7U-SzO2Tp_lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"The striped bats are hanging on their feet for best.\"\n",
        "\n",
        "tokens = word_tokenize(sample_sentence)\n",
        "lemma_words = [wnl.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(f'Lemmatised words: {lemma_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDVPN8yip8dK",
        "outputId": "1bf4cc87-e1fa-4332-8b46-c4bbdbd549c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised words: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If required, you can also convert the stem words into a sentence by merging them with a space between each word.\n",
        "print(f'Lemmatised sentence: {\" \".join(lemma_words)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDcw4N1lqiqJ",
        "outputId": "e943f721-f6a6-4e2f-e399-bc9b1d9b670b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised sentence: The striped bat are hanging on their foot for best .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy Lemmatization\n",
        "\n",
        "spaCy models are pipelines designed with multiple components.<br>\n",
        "You can find more details about available pipelines and models [here](https://spacy.io/models/en).\n"
      ],
      "metadata": {
        "id": "_Q5JYM-X_KDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm  # spacy model\n",
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "VBUHMc2Bsvoa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"The striped bats are hanging on their feet for best.\"\n",
        "\n",
        "# process a sentence using the spaCy pipeline\n",
        "doc = nlp(sample_sentence)\n",
        "# iterate through each token in the output document (processed sentence) and get its lemmatised version\n",
        "print([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GgHln8__2sU",
        "outputId": "c79d6199-8554-431d-cf75-82356867bbaa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'stripe', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'good', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFMZJc5oKZ_8"
      },
      "source": [
        "### <font color='green'>**Activity 2**</font>\n",
        "\n",
        "Fill the following table by applying WordNet and spaCy lemmatization to each given word.\n",
        "\n",
        "|Original word | Lemmatised word- WordNetLemmatizer  | Lemmatised word- spaCy |\n",
        "|------|--------------------|------------|\n",
        "|walking    | ? | ? |\n",
        "|is    | ? | ? |\n",
        "|main    | ? | ? |\n",
        "|animals    | ? | ? |\n",
        "|terrestrial    | ? | ? |\n",
        "|jumping    | ? | ? |\n",
        "|best    | ? | ? |\n",
        "|sleeping    | ? | ? |\n",
        "\n",
        "Which lemmatiser is the best to normalise text and why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBkMybM0JTpQ"
      },
      "source": [
        "### NLTK WordNetLemmatizer with Part-of-Speech (PoS) tags\n",
        "\n",
        "[Parts of speech](https://www.englishclub.com/grammar/parts-of-speech.htm) are also known as word classes or lexical categories.\n",
        "\n",
        "By feeding the corresponding PoS tag along with the word, we can further improve the WordNetLemmatizer.\n",
        "\n",
        "According the NLTK's [documentation](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.WordNetLemmatizer.lemmatize), “n” for nouns, “v” for verbs, “a” for adjectives and “r” for adverbs are the valid PoS tag options for WordNetLemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')  # NLTK module required for WordNetLemmatizer\n",
        "nltk.download('omw-1.4') # NLTK module required for WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')  #  NLTK module requried for PoS tagger\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqQrXogv0KOR",
        "outputId": "1de818fe-0639-4cbd-e8c0-422e664ca3f3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTrh7VoHJUdo",
        "outputId": "2f030093-43ed-4362-e6a4-198defeae79d"
      },
      "source": [
        "lemma_word=wnl.lemmatize('ponies', pos='n')\n",
        "print(lemma_word)\n",
        "\n",
        "lemma_word=wnl.lemmatize('walking', pos='v')\n",
        "print(lemma_word)\n",
        "\n",
        "lemma_word=wnl.lemmatize('better', pos='a')\n",
        "print(lemma_word)\n",
        "\n",
        "lemma_word=wnl.lemmatize('effectively', pos='r')\n",
        "print(lemma_word)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pony\n",
            "walk\n",
            "good\n",
            "effectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the PoSTagger available with [NLTK](https://www.nltk.org/api/nltk.tag.html) to automatically identify the PoS tags."
      ],
      "metadata": {
        "id": "1piMEHlZnY3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(['ponies', 'walking', 'best', 'effectively'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFzd_6iSj_bR",
        "outputId": "885f35f1-359a-404d-e14f-807bcb72b678"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ponies', 'NNS'), ('walking', 'VBG'), ('best', 'JJS'), ('effectively', 'RB')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNetLemmatizer requires PoS tags in the format of 'n', 'v', 'a' and 'r'.\n",
        "But, PoSTagger return tags in the format of 'NNS', 'VBG', 'JJS' and 'RB'.\n",
        "\n",
        "Let's write a simple function to get the PoS tag of a word in the format required by WordNetLemmatizer."
      ],
      "metadata": {
        "id": "MCnQP-j41MJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "1TsatnICrt8c"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are not familiar with what happens with Python dictionary get() method, find more details [here](https://www.w3schools.com/python/ref_dictionary_get.asp)."
      ],
      "metadata": {
        "id": "OAZi_OpamYiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = ['ponies', 'walking', 'best', 'effectively']\n",
        "\n",
        "for word in sample_words:\n",
        "  pos = get_wordnet_pos(word)\n",
        "  lemma_word=wnl.lemmatize(word, pos)\n",
        "  print(lemma_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0LihATls0So",
        "outputId": "a94b722f-3099-47d9-cb00-de6030c45056"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pony\n",
            "walk\n",
            "best\n",
            "effectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUJ6XKBKp-UM"
      },
      "source": [
        "# Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co78O5UX4lQa",
        "outputId": "bc6866fd-bbfe-4ed3-ce91-c000e43b97c8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You do not have to create a stopword list from scratch, as NLTK provides us with a readily available list. But you may need to update us depending on the problem you try to solve."
      ],
      "metadata": {
        "id": "5qhj0M_g46Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNJlwU3W5Zqa",
        "outputId": "c3d3e79b-e3fd-4d5f-bfd1-d018c9f966f3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"should've\", 'because', 'by', \"mightn't\", 'do', 'doing', 'have', 'having', 'to', 'when', 'few', 'no', 'he', \"wasn't\", 'their', 'nor', 'just', 'those', 'at', 'has', 'between', 'where', 'his', 'theirs', \"it's\", 'through', 'such', 'can', 'will', 'aren', 'hers', 'are', 'both', 'i', 'while', 'more', 'you', 'only', 'further', 'themselves', 'each', \"you're\", 'y', \"needn't\", 'than', 'them', 'ain', \"hadn't\", 'off', 'here', 'yours', 've', 'be', 'against', 'yourself', 'below', 'my', 'how', 'again', 'what', 'hasn', 'about', 'over', 'weren', 'before', 's', \"wouldn't\", 'o', 'they', 'if', \"weren't\", 'other', 'its', 'am', \"she's\", 't', 'after', 'of', 'there', 'shan', 'with', 'during', 'a', 'yourselves', 'needn', 'she', 'any', 'whom', 'don', 'should', 'most', 'these', 'won', 'had', 'why', 're', 'd', \"you'd\", 'did', 'from', 'too', 'been', 'which', 'out', 'own', 'in', 'her', 'is', 'up', 'on', 'once', 'and', 'not', \"hasn't\", 'same', \"isn't\", 'doesn', 'didn', 'isn', 'were', 'that', \"you'll\", 'shouldn', 'an', 'our', 'or', \"aren't\", 'wasn', 'your', 'wouldn', 'until', 'but', \"don't\", \"haven't\", \"shouldn't\", 'ma', 'me', \"shan't\", 'm', \"didn't\", 'himself', 'herself', 'we', 'so', 'for', 'very', \"couldn't\", \"mustn't\", 'ours', 'does', \"doesn't\", 'it', \"that'll\", 'all', 'mustn', 'itself', \"you've\", 'was', 'now', 'down', 'll', 'as', 'into', 'couldn', 'him', 'haven', \"won't\", 'under', 'then', 'being', 'hadn', 'mightn', 'myself', 'ourselves', 'who', 'the', 'above', 'some', 'this'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to remove stopwords in a sentence."
      ],
      "metadata": {
        "id": "7KBEDG3i5sBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"This is a sample sentence, showing off the stop words removal.\"\n",
        "\n",
        "# tokenise text\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# remove stopwords from tokens\n",
        "filtered_words = [token for token in tokens if token not in stop_words]\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "id": "wrCkf5fg8cVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049b1a55-231d-426e-ccd6-741b7f314ea7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='green'>**Activity 3**</font>\n",
        "\n",
        "Update the default stop word list by removing 'off', and remove stop words in the sample_text.\n",
        "\n",
        "**Expected output:** \\['This', 'sample', 'sentence', ',', 'showing', 'off', 'stop', 'words', 'removal', '.']\n",
        "\n",
        "**Hint:** [Python - Remove List Items](https://www.w3schools.com/python/python_lists_remove.asp)"
      ],
      "metadata": {
        "id": "Pz8L00n9J6gV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OParvOihqaK4"
      },
      "source": [
        "# Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "XmcF_eE86-Xk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can get a readily available set of punctuations using the Python string package."
      ],
      "metadata": {
        "id": "ihXs9OLs7Egi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Punctuation marks: {string.punctuation}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeBf46Jv7aKc",
        "outputId": "03ad6054-4173-47f0-851b-e1b919e192a1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation marks: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Let's remove punctuation marks!\"\n",
        "\n",
        "# remove puncuation marks in sample text\n",
        "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "no_punctuation= sample_text.translate(table)\n",
        "\n",
        "print(no_punctuation)"
      ],
      "metadata": {
        "id": "IocTzFuv-Q0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b790297-6b7c-4556-fe1b-6f866be07513"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lets remove punctuation marks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)\n",
        "\n",
        "Let's see how to use [spaCy](https://spacy.io/usage/linguistic-features#named-entities) models for NER.\n",
        "\n",
        "[spaCy English Models](https://spacy.io/models/en)"
      ],
      "metadata": {
        "id": "hNuUElQSzvm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm  # spacy model\n",
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "YmZH1stw8S1g"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Apple is looking at buying U.K. startup for $1 billion\""
      ],
      "metadata": {
        "id": "Mq0ULRvWzuyF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sample_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "izGIa-ai1cI-",
        "outputId": "f6f56062-c7ce-494c-b424-43b64a19af4e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is looking at buying \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " startup for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace text with recognised named entities."
      ],
      "metadata": {
        "id": "rEmztv4PGTSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sample_text)\n",
        "\n",
        "updated_tokens = [t.text if not t.ent_type_ else t.ent_type_ for t in doc]\n",
        "updated_sentence = \" \".join(updated_tokens)\n",
        "print(updated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwYS5IUhGaij",
        "outputId": "722df931-c50a-443d-c59a-8eb0f4b214b9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG is looking at buying GPE startup for MONEY MONEY MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repitetions of the same named entity can be merged by adding ['merge_entities'](https://spacy.io/api/pipeline-functions#merge_entities) to the pipeline."
      ],
      "metadata": {
        "id": "tlpTMJD-H3EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"merge_entities\")\n",
        "\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "updated_tokens = [t.text if not t.ent_type_ else t.ent_type_ for t in doc]\n",
        "updated_sentence = \" \".join(updated_tokens)\n",
        "print(updated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQjhekyzH1XE",
        "outputId": "7d8ffbf8-daa5-4bf9-8d67-1fec97d2aca3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG is looking at buying GPE startup for MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='green'>**Activity 4**</font>\n",
        "\n",
        "Let's assume you need to identify the sentiment (positive, negative and neutral) of a given product review. A few sample reviews are given bellow.\n",
        "\n",
        "* \"Apple's new product is amazing.\"\n",
        "* \"I'm quite dissapointed with recent Apple products.\"\n",
        "* \"Android products are amazing and versatile.\"\n",
        "\n",
        "a) Replace the entities in these sentences using entity tags.\n",
        "\n",
        "b) Would this replacement be helpful for sentiment identification from the perpective of a machine learning model?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VYUfy6jy8qrb"
      }
    }
  ]
}