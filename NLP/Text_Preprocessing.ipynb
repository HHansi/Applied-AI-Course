{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HHansi/Applied-AI-Course/blob/main/NLP/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bijgqBg-VMJA"
      },
      "source": [
        "# Text Processing\n",
        "\n",
        "This notebook contains the practical examples and exercises for the Applied AI-Natural Language Processing.\n",
        "\n",
        "*Created by Hansi Hettiarachchi*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "qXcA1jY8Q0Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# download NLTK modules\n",
        "nltk.download('punkt')  # required for Tokenizers\n",
        "nltk.download('wordnet')  # required for WordNetLemmatizer\n",
        "nltk.download('omw-1.4') # required for WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')  # requried for PoS tagger\n",
        "nltk.download('stopwords')  \n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm  # spacy model\n",
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLhPY5Ffnh45",
        "outputId": "b5be6470-b452-4bfb-ea91-0d1ddd518ff4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenisation\n",
        "Tokenisation is the process which divides text into smaller parts called tokens.\n",
        "\n",
        "Let's see how to use [tokenizers](https://www.nltk.org/api/nltk.tokenize.html) available with NLTK (Natural Language Toolkit) package to tokenise text."
      ],
      "metadata": {
        "id": "0yTL8Gxi6jrE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF5HAhIugmwP"
      },
      "source": [
        "sample_text = \"This is a sentence, which contains all kind of words, and needs to be tokenized!\"\n",
        "sample_tweet1 = \"This is a cooool :-) :-P <3 #cool\"\n",
        "sample_tweet2 = \"@remy: This is waaaaayyyy too much for you!!!!!!\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj8A08V7h1ir"
      },
      "source": [
        "Tokenising normal text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40Dcb7ITfT2y",
        "outputId": "f71ce255-e699-4bb4-8daa-64b5a64b1ede"
      },
      "source": [
        "tokenized_text = word_tokenize(sample_text)\n",
        "print(tokenized_text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sentence', ',', 'which', 'contains', 'all', 'kind', 'of', 'words', ',', 'and', 'needs', 'to', 'be', 'tokenized', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PlF7OQh73p"
      },
      "source": [
        "Tokenising tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAtt6c63VOU5",
        "outputId": "58d5eb9d-92d5-473b-df23-221261f7fd77"
      },
      "source": [
        "tokenized_tweet1 = word_tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = word_tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized tweet1: ['This', 'is', 'a', 'cooool', ':', '-', ')', ':', '-P', '<', '3', '#', 'cool']\n",
            "tokenized tweet2: ['@', 'remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3MnOMdrh-da"
      },
      "source": [
        "As you can see in the above outputs, <i>word_tokenize</i> cannot tokenize the tweet text correctly. \n",
        "Considering the differences in tweet text compared to normal text, there is an another tokenizer named <i>TweetTokenizer</i> available with NLTK which is specifically designed for tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8LHQrQrfllV",
        "outputId": "a874b0d3-aaf2-4f24-c54f-b01cff0fee31"
      },
      "source": [
        "tknzr = TweetTokenizer()\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized tweet1: ['This', 'is', 'a', 'cooool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-KNtZ6LjSvA"
      },
      "source": [
        "Let's analyse more features available with [TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.casual.html?highlight=tweettokenizer#nltk.tokenize.casual.TweetTokenizer).\n",
        "- preserve_case (default setting=True) - Keep case sensitivity of the text\n",
        "- reduce_len (default setting=False) - Normalize text by removing repeated character sequences of length 3 or greater with sequences of length 3.\n",
        "- strip_handles (default setting=False) - Remove Twitter usernames in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv2rAXcHXqON",
        "outputId": "efb27bda-beb9-48bb-9267-76e8673ce990"
      },
      "source": [
        "# make the tokens case insensitive or convert into lowercase\n",
        "print('configs: preserve_case=False')\n",
        "tknzr = TweetTokenizer(preserve_case=False)\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')\n",
        "\n",
        "\n",
        "# make the tokens case insensitive and reduce length\n",
        "print('\\nconfigs: preserve_case=False, reduce_len=True')\n",
        "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')\n",
        "\n",
        "\n",
        "# make the tokens case insensitive, reduce length and remove usernames\n",
        "print('\\nconfigs: preserve_case=False, reduce_len=True, strip_handles=True')\n",
        "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "configs: preserve_case=False\n",
            "tokenized tweet1: ['this', 'is', 'a', 'cooool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'this', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
            "\n",
            "configs: preserve_case=False, reduce_len=True\n",
            "tokenized tweet1: ['this', 'is', 'a', 'coool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'this', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
            "\n",
            "configs: preserve_case=False, reduce_len=True, strip_handles=True\n",
            "tokenized tweet1: ['this', 'is', 'a', 'coool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: [':', 'this', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qkRlhEoH6IC"
      },
      "source": [
        "# Text Normalisation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oTfsvegJn5C"
      },
      "source": [
        "## Lower casing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"The striped BATs are hanging on their feet for best\""
      ],
      "metadata": {
        "id": "vRRSaEGpnBmg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1V46nFRRcv",
        "outputId": "628c213d-2b61-4b4b-ec66-513bceee4b23"
      },
      "source": [
        "# Any string can be lower cased using the function lower()\n",
        "lower_cased_text = sample_text.lower()\n",
        "print(lower_cased_text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the striped bats are hanging on their feet for best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are not familiar with string methods, you can find a list of all of them in the [documentation](https://docs.python.org/3.7/library/stdtypes.html#string-methods)."
      ],
      "metadata": {
        "id": "yrVsQP4qi-SX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9M2IprIg2H"
      },
      "source": [
        "## Stemming\n",
        "Stemming chops off the end or beginning of words by taking into account a list of common prefixes or suffixes that could be found in that word.\n",
        "\n",
        "The most common and effecive algorithm for stemming English is <i>Porter’s algorithm.</i>\n",
        "\n",
        "[Stemmers in NLTK](https://www.nltk.org/howto/stem.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = [\"dogs\", \"ponies\", \"eating\", \"corpora\"]\n",
        "sample_sentence = \"The striped bats are hanging on their feet for best.\""
      ],
      "metadata": {
        "id": "DPiEiZ69nRyw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vIsy8ZcIftF",
        "outputId": "fbe1f424-5ad4-4182-a463-9ae78be64b57"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "stem_words = [ps.stem(word) for word in sample_words]\n",
        "print(f'Stemmed words: {stem_words}\\n')\n",
        "\n",
        "stem_words = [ps.stem(word) for word in word_tokenize(sample_sentence)]\n",
        "print(f'Stemmed words: {stem_words}')\n",
        "print(f'Stemmed sentence: {\" \".join(stem_words)}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['dog', 'poni', 'eat', 'corpora']\n",
            "\n",
            "Stemmed words: ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best', '.']\n",
            "Stemmed sentence: the stripe bat are hang on their feet for best .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsDEnGi2I4GT"
      },
      "source": [
        "## Lemmatisation\n",
        "\n",
        "Lemmatisation is an more organised procedure to obtain the base form of a word (lemma) with the use of a vocabulary and morphological analysis (word structure and grammar relations) of words."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.WordNetLemmatizer.lemmatize)"
      ],
      "metadata": {
        "id": "hQx5xW5NkKWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = [\"dogs\", \"ponies\", \"eating\", \"corpora\"]\n",
        "sample_sentence = \"The striped bats are hanging on their feet for best.\""
      ],
      "metadata": {
        "id": "keYTm1zKjcNG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWZTVWOvH-5m",
        "outputId": "9212befc-a4f0-4447-c19d-fe932c7ab96c"
      },
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "lemma_words = [wnl.lemmatize(word) for word in sample_words]\n",
        "print(f'Lemmatised words: {lemma_words}\\n')\n",
        "\n",
        "lemma_words = [wnl.lemmatize(word) for word in word_tokenize(sample_sentence)]\n",
        "print(f'Lemmatised words: {lemma_words}')\n",
        "print(f'Lemmatised sentence: {\" \".join(lemma_words)}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised words: ['dog', 'pony', 'eating', 'corpus']\n",
            "\n",
            "Lemmatised words: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best', '.']\n",
            "Lemmatised sentence: The striped bat are hanging on their foot for best .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFMZJc5oKZ_8"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "Compare the difference between the outputs by stemmer and lemmatiser.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBkMybM0JTpQ"
      },
      "source": [
        "### NLTK WordNetLemmatizer with Part-of-Speech (PoS) tags\n",
        "\n",
        "[Parts of speech](https://www.englishclub.com/grammar/parts-of-speech.htm) are also known as word classes or lexical categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTrh7VoHJUdo",
        "outputId": "759e8080-af93-4464-bfde-e1b2d5bc4e74"
      },
      "source": [
        "lemma_word=wnl.lemmatize('ponies', pos='n')\n",
        "print(lemma_word)\n",
        "\n",
        "lemma_word=wnl.lemmatize('eating', pos='v')\n",
        "print(lemma_word)\n",
        "\n",
        "lemma_word=wnl.lemmatize('ate', pos='v')\n",
        "print(lemma_word)\n",
        "\n",
        "lemma_word=wnl.lemmatize('better', pos='a')\n",
        "print(lemma_word)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pony\n",
            "eat\n",
            "eat\n",
            "good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NLTK PoS Taggers](https://www.nltk.org/api/nltk.tag.html)"
      ],
      "metadata": {
        "id": "1piMEHlZnY3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(sample_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFzd_6iSj_bR",
        "outputId": "a878c23e-491f-4710-dc5e-f8e5f42b243f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dogs', 'NNS'), ('ponies', 'NNS'), ('eating', 'VBG'), ('corpora', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "1TsatnICrt8c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are not familiar with what happens with Python dictionary get() method, find more details [here](https://www.w3schools.com/python/ref_dictionary_get.asp)."
      ],
      "metadata": {
        "id": "OAZi_OpamYiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_words = [wnl.lemmatize(word, pos=get_wordnet_pos(word)) for word in sample_words]\n",
        "print(f'Lemmatised words: {lemma_words}\\n')\n",
        "\n",
        "lemma_words = [wnl.lemmatize(word) for word in word_tokenize(sample_sentence)]\n",
        "print(f'Lemmatised words: {lemma_words}')\n",
        "print(f'Lemmatised sentence: {\" \".join(lemma_words)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0LihATls0So",
        "outputId": "0074f9bb-7693-49cb-d5ea-367ec1f04a85"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised words: ['dog', 'pony', 'eat', 'corpus']\n",
            "\n",
            "Lemmatised words: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best', '.']\n",
            "Lemmatised sentence: The striped bat are hanging on their foot for best .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy Lemmatization\n",
        "\n",
        "spaCy models are pipelines designed with multiple components.<br>\n",
        "[spaCy English Models](https://spacy.io/models/en)\n"
      ],
      "metadata": {
        "id": "_Q5JYM-X_KDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sample_sentence)\n",
        "print([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GgHln8__2sU",
        "outputId": "647374aa-dc9a-43f6-c634-82efdbf2828d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'stripe', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'good', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "Compare the lemmatised outputs generated by WordNetLemmatizer and spaCy Lemmatizer."
      ],
      "metadata": {
        "id": "AJKiLLiVJfVX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUJ6XKBKp-UM"
      },
      "source": [
        "# Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"This is a sample sentence, showing off the stop words removal.\""
      ],
      "metadata": {
        "id": "wrCkf5fg8cVY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxljnBblp9IS",
        "outputId": "617be0cd-7e52-4e09-9f87-1c4d9a2d39fd"
      },
      "source": [
        "# define set of English stopwords\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# tokenise text\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# remove stopwords from tokens\n",
        "filtered_words = [token for token in tokens if token not in stop_words]\n",
        "print(filtered_words)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "Update the default stop word list by removing 'off', and remove stop words in the sample_text.\n",
        "\n",
        "expected output = \\['This', 'sample', 'sentence', ',', 'showing', 'off', 'stop', 'words', 'removal', '.']"
      ],
      "metadata": {
        "id": "Pz8L00n9J6gV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OParvOihqaK4"
      },
      "source": [
        "# Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Let's remove punctuation marks!\""
      ],
      "metadata": {
        "id": "IocTzFuv-Q0C"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnZQE1FpqZUF",
        "outputId": "eb0b58f8-1d28-4c61-90e2-0067bc042b8a"
      },
      "source": [
        "print(f'Punctuation marks: {string.punctuation}\\n')\n",
        "\n",
        "# remove puncuation marks in sample text\n",
        "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "no_punctuation= sample_text.translate(table)\n",
        "\n",
        "print(no_punctuation)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation marks: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "\n",
            "Lets remove punctuation marks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)\n",
        "\n",
        "Let's see how to use [spaCy](https://spacy.io/usage/linguistic-features#named-entities) models for NER.\n",
        "\n",
        "[spaCy English Models](https://spacy.io/models/en)"
      ],
      "metadata": {
        "id": "hNuUElQSzvm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Apple is looking at buying U.K. startup for $1 billion\""
      ],
      "metadata": {
        "id": "Mq0ULRvWzuyF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sample_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "izGIa-ai1cI-",
        "outputId": "1d9c4b6c-1d0c-4d72-e215-8ed0bb75cc14"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is looking at buying \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " startup for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace text with recognised named entities."
      ],
      "metadata": {
        "id": "rEmztv4PGTSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sample_text)\n",
        "\n",
        "updated_tokens = [t.text if not t.ent_type_ else t.ent_type_ for t in doc]\n",
        "updated_sentence = \" \".join(updated_tokens)\n",
        "print(updated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwYS5IUhGaij",
        "outputId": "294293d1-0bb7-46ad-df60-bb5329cee356"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG is looking at buying GPE startup for MONEY MONEY MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repitetions of the same named entity can be merged by adding ['merge_entities'](https://spacy.io/api/pipeline-functions#merge_entities) to the pipeline."
      ],
      "metadata": {
        "id": "tlpTMJD-H3EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"merge_entities\")\n",
        "\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "updated_tokens = [t.text if not t.ent_type_ else t.ent_type_ for t in doc]\n",
        "updated_sentence = \" \".join(updated_tokens)\n",
        "print(updated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQjhekyzH1XE",
        "outputId": "01eec40f-9919-47ad-ade1-91facd1e8c40"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG is looking at buying GPE startup for MONEY\n"
          ]
        }
      ]
    }
  ]
}